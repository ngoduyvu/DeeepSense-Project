{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ast\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the sentence\n",
    "def preProcessing(inputFileStr,outputFileStr,printResult):\n",
    "    inputFile = open(inputFileStr,\"r\").read()\n",
    "    outputFile=open (outputFileStr,\"w+\")\n",
    "    cachedStopWords = nltk.corpus.stopwords.words(\"english\")\n",
    "    cachedStopWords.append('OMG')\n",
    "    cachedStopWords.append(':-)')\n",
    "    result=(' '.join([word for word in inputFile.split() if word not in cachedStopWords]))\n",
    "    if(printResult):\n",
    "        print('Following are the Stop Words')\n",
    "        print(cachedStopWords)\n",
    "        print(str(result))\n",
    "    outputFile.write(str(result))\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preProcessing('ReviewDataset.txt', 'PreProcessedData.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeReviews(inputFileStr,outputFileStr,printResult):\n",
    "    tokenizedReviews={}\n",
    "    inputFile = open(inputFileStr,\"r\").read()\n",
    "    outputFile=open (outputFileStr,\"w\")\n",
    "    tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "    uniqueId=1;\n",
    "    cachedStopWords = nltk.corpus.stopwords.words(\"english\")\n",
    "    for sentence in tokenizer.tokenize(inputFile):      \n",
    "        tokenizedReviews[uniqueId]=sentence\n",
    "        uniqueId+=1\n",
    "    outputFile.write(str(tokenizedReviews))\n",
    "    if(printResult):\n",
    "        for key,value in tokenizedReviews.items():\n",
    "            print(key,' ',value)\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizeReviews('PreProcessedData.txt', 'TokenizedReviews.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posTagging(inputFileStr,outputFileStr,printResult):\n",
    "    inputFile = open(inputFileStr,\"r\").read()\n",
    "    outputFile=open (outputFileStr,\"w\")\n",
    "    inputTupples=ast.literal_eval(inputFile)\n",
    "    outputPost={}\n",
    "    for key,value in inputTupples.items():\n",
    "        outputPost[key]=nltk.pos_tag(nltk.word_tokenize(value))\n",
    "    if(printResult):\n",
    "        for key,value in outputPost.items():\n",
    "            print(key,' ',value)\n",
    "    outputFile.write(str(outputPost))\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "posTagging('TokenizedReviews.txt', 'PosTaggedReviews.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aspectExtraction(inputFileStr,outputFileStr,printResult):\n",
    "    inputFile = open(inputFileStr,\"r\").read()\n",
    "    outputFile=open (outputFileStr,\"w\")\n",
    "    inputTupples=ast.literal_eval(inputFile)\n",
    "    prevWord=''\n",
    "    prevTag=''\n",
    "    currWord=''\n",
    "    aspectList=[]\n",
    "    outputDict={}\n",
    "    #Extracting Aspects\n",
    "    for key,value in inputTupples.items():\n",
    "        for word,tag in value:\n",
    "            if(tag=='NN' or tag=='NNP'):\n",
    "                if(prevTag=='NN' or prevTag=='NNP'):\n",
    "                    currWord= prevWord + ' ' + word\n",
    "                else:\n",
    "                    aspectList.append(prevWord.upper())\n",
    "                    currWord= word\n",
    "            prevWord=currWord\n",
    "            prevTag=tag\n",
    "    #Eliminating aspect which has 1 or less count\n",
    "    for aspect in aspectList:\n",
    "            if(aspectList.count(aspect)>1):\n",
    "                    if(outputDict.keys()!=aspect):\n",
    "                            outputDict[aspect]=aspectList.count(aspect)\n",
    "    outputAspect=sorted(outputDict.items(), key=lambda x: x[1],reverse = True)\n",
    "    if(printResult):\n",
    "        print(outputAspect)\n",
    "    outputFile.write(str(outputAspect))\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "aspectExtraction('PosTaggedReviews.txt', 'Aspects.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identifyOpinionWords(inputReviewListStr, inputAspectListStr, outputAspectOpinionListStr,printResult):       \n",
    "    inputReviewList = open(inputReviewListStr,\"r\").read()\n",
    "    inputAspectList = open(inputAspectListStr,\"r\").read()\n",
    "    outputAspectOpinionList=open (outputAspectOpinionListStr,\"w\")\n",
    "    inputReviewsTuples=ast.literal_eval(inputReviewList)\n",
    "    inputAspectTuples=ast.literal_eval(inputAspectList)\n",
    "    outputAspectOpinionTuples={}\n",
    "    orientationCache={}\n",
    "    negativeWordSet = {\"don't\",\"never\", \"nothing\", \"nowhere\", \"noone\", \"none\", \"not\",\n",
    "                  \"hasn't\",\"hadn't\",\"can't\",\"couldn't\",\"shouldn't\",\"won't\",\n",
    "                  \"wouldn't\",\"don't\",\"doesn't\",\"didn't\",\"isn't\",\"aren't\",\"ain't\"}\n",
    "    for aspect,no in inputAspectTuples:\n",
    "        aspectTokens= word_tokenize(aspect)\n",
    "        count=0\n",
    "        for key,value in inputReviewsTuples.items():\n",
    "            condition=True\n",
    "            isNegativeSen=False\n",
    "            for subWord in aspectTokens:\n",
    "                if(subWord in str(value).upper()):\n",
    "                    condition = condition and True\n",
    "                else:\n",
    "                    condition = condition and False\n",
    "            if(condition):\n",
    "                for negWord in negativeWordSet:\n",
    "                    if(not isNegativeSen):#once senetence is negative no need to check this condition again and again\n",
    "                        if negWord.upper() in str(value).upper():\n",
    "                            isNegativeSen=isNegativeSen or True\n",
    "                outputAspectOpinionTuples.setdefault(aspect,[0,0,0])\n",
    "                for word,tag in value:\n",
    "                     if(tag=='JJ' or tag=='JJR' or tag=='JJS'or tag== 'RB' or tag== 'RBR'or tag== 'RBS'):\n",
    "                         count+=1\n",
    "                         if(word not in orientationCache):\n",
    "                             orien=orientation(word)\n",
    "                             orientationCache[word]=orien\n",
    "                         else:\n",
    "                             orien=orientationCache[word]\n",
    "                         if(isNegativeSen and orien is not None):\n",
    "                             orien= not orien\n",
    "                         if(orien==True):\n",
    "                             outputAspectOpinionTuples[aspect][0]+=1\n",
    "                         elif(orien==False):\n",
    "                             outputAspectOpinionTuples[aspect][1]+=1\n",
    "                         elif(orien is None):\n",
    "                             outputAspectOpinionTuples[aspect][2]+=1\n",
    "        if(count>0):\n",
    "            #print(aspect,' ', outputAspectOpinionTuples[aspect][0], ' ',outputAspectOpinionTuples[aspect][1], ' ',outputAspectOpinionTuples[aspect][2])\n",
    "            outputAspectOpinionTuples[aspect][0]=round((outputAspectOpinionTuples[aspect][0]/count)*100,2)\n",
    "            outputAspectOpinionTuples[aspect][1]=round((outputAspectOpinionTuples[aspect][1]/count)*100,2)\n",
    "            outputAspectOpinionTuples[aspect][2]=round((outputAspectOpinionTuples[aspect][2]/count)*100,2)\n",
    "            print(aspect,':\\t\\tPositive => ', outputAspectOpinionTuples[aspect][0], '\\tNegative => ',outputAspectOpinionTuples[aspect][1])\n",
    "    if(printResult):\n",
    "        print(outputAspectOpinionList)\n",
    "    outputAspectOpinionList.write(str(outputAspectOpinionTuples))\n",
    "    outputAspectOpinionList.close();\n",
    "#-----------------------------------------------------------------------------------\n",
    "def orientation(inputWord): \n",
    "    wordSynset=wordnet.synsets(inputWord)\n",
    "    if(len(wordSynset) != 0):\n",
    "        word=wordSynset[0].name()\n",
    "        orientation=sentiwordnet.senti_synset(word)\n",
    "        if(orientation.pos_score()>orientation.neg_score()):\n",
    "            return True\n",
    "        elif(orientation.pos_score()<orientation.neg_score()):\n",
    "            return False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "identifyOpinionWords('PosTaggedReviews.txt','Aspects.txt', 'Output.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printResultChoice():\n",
    "    userChoice = str(input('\\nDo you want to print the result on output window? (Y/N) :'))\n",
    "    if(userChoice=='Y' or userChoice=='y'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "ReviewDataset = 'ReviewDataset.txt'\n",
    "PreProcessedData = 'PreProcessedData.txt'\n",
    "TokenizedReviews = 'TokenizedReviews.txt'\n",
    "PosTaggedReviews = 'PosTaggedReviews.txt'\n",
    "Aspects = 'Aspects.txt'\n",
    "Opinions = 'Opinions.txt'\n",
    "\n",
    "print(\"\\nWELCOME TO OPINION MINING SYSTEM  \")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "input(\"Please Enter any key to continue...\")\n",
    "print(\"\\n\\n\\n\\n\\n\\nPREPROCESSING DATA\")\n",
    "omsFunctions.preProcessing(_ReviewDataset,_PreProcessedData,printResultChoice())\n",
    "print(\"\\n\\n\\n\\n\\n\\nREADING REVIEW COLLECTION...\")\n",
    "omsFunctions.tokenizeReviews(_ReviewDataset,_TokenizedReviews,printResultChoice())\n",
    "print(\"\\n\\n\\n\\n\\n\\nPART OF SPEECH TAGGING...\")\n",
    "omsFunctions.posTagging(_TokenizedReviews,_PosTaggedReviews,printResultChoice())\n",
    "print(\"\\nThis function will list all the nouns as aspect\")\n",
    "omsFunctions.aspectExtraction(_PosTaggedReviews,_Aspects,printResultChoice())\n",
    "print(\"\\n\\n\\n\\n\\n\\nIDENTIFYING OPINION WORDS...\")\n",
    "omsFunctions.identifyOpinionWords(_PosTaggedReviews,_Aspects,_Opinions,printResultChoice())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
