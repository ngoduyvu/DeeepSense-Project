{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s4341237\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old data size: 98188\n",
      "New data size: 146376\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>category</th>\n",
       "      <th>score</th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IQYGB0H03MVU</td>\n",
       "      <td>B00021XIJW</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>great way boost wireless range picked linksys ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A31RNXQHD106YY</td>\n",
       "      <td>60516054</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>smug ethnocentrism book smug ethnocentrism sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT7W1EXT0PF2Y</td>\n",
       "      <td>471272426</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>skeptic dictionary warning review contains mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1MH1V09NGMJ2P</td>\n",
       "      <td>B000BVCSMG</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>power conditioning important snake oil first a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2ZMI5UAMCTQHA</td>\n",
       "      <td>B0013A1XDE</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>compared note updated review note amateur thin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      reviewer_id        asin  overall  category     score  good  bad  \\\n",
       "0  A2IQYGB0H03MVU  B00021XIJW        4        10  0.990385     1    0   \n",
       "1  A31RNXQHD106YY    60516054        3         5  0.500000     0    0   \n",
       "2   AT7W1EXT0PF2Y   471272426        1         5  0.197531     0    1   \n",
       "3  A1MH1V09NGMJ2P  B000BVCSMG        5        10  0.847222     1    0   \n",
       "4  A2ZMI5UAMCTQHA  B0013A1XDE        5        10  0.967213     1    0   \n",
       "\n",
       "                                         review_text  \n",
       "0  great way boost wireless range picked linksys ...  \n",
       "1  smug ethnocentrism book smug ethnocentrism sho...  \n",
       "2  skeptic dictionary warning review contains mas...  \n",
       "3  power conditioning important snake oil first a...  \n",
       "4  compared note updated review note amateur thin...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_train_test(df, test_size):\n",
    "    train, test = train_test_split(df, test_size=test_size, random_state=random.randint(0, 99))\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    return train, test\n",
    "\n",
    "dataset_path = 'new_data_william.csv' # Minimized dataset\n",
    "\n",
    "df = pd.read_csv(dataset_path, sep=',', index_col=0)\n",
    "print(\"Old data size:\", len(df))\n",
    "df_majority = df[df.good==0]\n",
    "df_minority = df[df.good==1]\n",
    "df_minority_upsampled = resample(df_minority, replace=True, \n",
    "        n_samples=len(df_majority), random_state=123)\n",
    "df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "del(df_majority)\n",
    "del(df_minority)\n",
    "del(df_minority_upsampled)\n",
    "\n",
    "# Training: 60%, Validation: 20%, Testing: 20%\n",
    "df_train, df_test = split_train_test(df, 0.200)\n",
    "df_train, df_val = split_train_test(df_train, 0.250)\n",
    "print(\"New data size:\", len(df))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tf-idf featurizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.97183459,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.0385293 ,  0.        ],\n",
       "       [ 0.93216633,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.95128994,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.97934358,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.01310616,  0.        ],\n",
       "       [ 0.937278  ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.9526439 ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.06239837]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TfidfFeaturizer(object):\n",
    "    def __init__(self, col_name='review_text', max_features=1000):\n",
    "        self.col_name = col_name\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = None\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        docs = self.__create_doc_list(df)\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(0,3),\n",
    "                                          analyzer='word',\n",
    "                                          token_pattern='[a-zA-Z0-9]+', \n",
    "                                          tokenizer=word_tokenize,\n",
    "                                          max_features=self.max_features)\n",
    "        return (self.vectorizer.fit_transform(docs)).toarray()\n",
    "\n",
    "    def transform(self, df):\n",
    "        if self.vectorizer is not None:\n",
    "            docs = self.__create_doc_list(df)\n",
    "            return (self.vectorizer.transform(docs)).toarray()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __create_doc_list(self, df):\n",
    "        return df[self.col_name].tolist()\n",
    "\n",
    "n_tfidf_features = 500\n",
    "tfidf_featurizer = TfidfFeaturizer(max_features=n_tfidf_features)\n",
    "\n",
    "print('Fitting tf-idf featurizer.')\n",
    "tfidf_featurizer.fit_transform(df)\n",
    "\n",
    "#print('Featurizing training set.')\n",
    "#tfidf_train = tfidf_featurizer.transform(df_train)\n",
    "#n_train = tfidf_train.shape[0]\n",
    "#print('Shape:', tfidf_train.shape)\n",
    "\n",
    "#print('Featurizing validation set.')\n",
    "#tfidf_val = tfidf_featurizer.transform(df_val)\n",
    "#n_val = tfidf_val.shape[0]\n",
    "#print('Shape:', tfidf_val.shape)\n",
    "\n",
    "#print('Featurizing test set.')\n",
    "#tfidf_test = tfidf_featurizer.transform(df_test)\n",
    "#n_test = tfidf_test.shape[0]\n",
    "#print('Shape:', tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_train = tfidf_train\n",
    "matrix_val   = tfidf_val\n",
    "matrix_test  = tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure labels\n",
    "target_class = 'good'\n",
    "\n",
    "labels_train = np.array(df_train[target_class])\n",
    "labels_val = np.array(df_val[target_class])\n",
    "labels_test = np.array(df_test[target_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(tfidf_train)\n",
    "del(tfidf_val)\n",
    "del(tfidf_test)\n",
    "del(df)\n",
    "del(df_train)\n",
    "del(df_val)\n",
    "del(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=500, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(matrix_train, labels_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          verbose=2,\n",
    "          validation_data=(matrix_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vectorizer_v3.pk', 'wb') as fin:\n",
    "    pickle.dump(tfidf_featurizer, fin, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bad_review = pd.read_csv('Dataset/data_wei_kitchen_0.3.csv', sep=',', index_col=0)\n",
    "tfidf_bad_review = tfidf_featurizer.transform(bad_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model_v3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model_v3.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review['predict_prob'] = model.predict(tfidf_bad_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056/1107 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "bad_review['predict_classes'] = model.predict_classes(tfidf_bad_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8590785907859079\n"
     ]
    }
   ],
   "source": [
    "counter_0 = 0\n",
    "for i in bad_review['predict_classes']:\n",
    "    if i == 0:\n",
    "        counter_0 = counter_0 + 1\n",
    "print(counter_0/len(bad_review))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review.to_csv('bad_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32800/32920 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "good_review = pd.read_csv('Dataset/data_wei_kitchen_0.8.csv', sep=',', index_col=0)\n",
    "tfidf_good_review = tfidf_featurizer.transform(good_review)\n",
    "good_review['predict_prob'] = model.predict(tfidf_good_review)\n",
    "good_review['predict_classes'] = model.predict_classes(tfidf_good_review)\n",
    "good_review.to_csv('good_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7135479951397327\n"
     ]
    }
   ],
   "source": [
    "counter_1 = 0\n",
    "for i in good_review['predict_classes']:\n",
    "    if i == 1:\n",
    "        counter_1 = counter_1 + 1\n",
    "print(counter_1/len(good_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
